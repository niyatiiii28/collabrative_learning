{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CLEAN START \n%cd /kaggle/working\n\n# Clone OpenFace repo\n!git clone https://github.com/TadasBaltrusaitis/OpenFace.git\n%cd OpenFace\n\n# Download pre-trained models (required for FeatureExtraction)\n!bash download_models.sh\n\n# Install dependencies\n!apt-get update -y\n!apt-get install -y build-essential cmake libopenblas-dev liblapack-dev \\\n    libboost-all-dev libx11-dev libpng-dev libjpeg-dev\n\n# Clone and build dlib (C++ version)\n!git clone https://github.com/davisking/dlib.git\n%cd dlib\n!mkdir build\n%cd build\n!cmake ..\n!cmake --build . --config Release\n!make install\n%cd /kaggle/working/OpenFace\n\n# Build OpenFace (this is the important corrected step!)\n!mkdir build\n%cd build\n!cmake ..\n!make -j4\n%cd /kaggle/working","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -l /kaggle/working/OpenFace/build/bin/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!/usr/bin/env python3\n# openface_gazelle_hybrid_with_logs_full.py\n# Full pipeline: OpenFace (face bbox CSVs) + GazeLLE (heatmaps + in/out)\n# Produces feature matrices, per-frame logs, partner/elsewhere events, and annotated videos.\n#\n# Edit the VIDEO* / OUTDIR / OPENFACE_BIN paths below as needed for your Kaggle environment.\n\nimport os\nimport sys\nimport csv\nimport math\nimport time\nimport glob\nfrom typing import Optional, List, Tuple, Dict, Any\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageDraw, ImageFont\nimport imageio.v3 as iio\nimport matplotlib.cm as cm\nimport torch\n\n# ---------------- USER PATHS (edit) ----------------\nVIDEO1 = \"/kaggle/input/project-videos/project_videos/p_17/camera1_output.mp4\"\nVIDEO2 = \"/kaggle/input/project-videos/project_videos/p_17/camera2_output.mp4\"\nOUTDIR = \"/kaggle/working/gaze_outputs\"\nOPENFACE_BIN = \"/kaggle/working/OpenFace/build/bin/FeatureExtraction\"\n# --------------------------------------------------\n\n# ---------------- PARAMETERS (tweak) ----------------\nINOUT_THRESH = 0.5\nSMOOTH_WINDOW = 3\nFIXATION_RADIUS_NORM = 0.03\nMOTION_THRESH_NORM = 0.02\nWORKSPACE_Y_MIN = 0.6\nPARTNER_BOX_EXPAND = 1.5\nJOINT_GAZE_DISTANCE_NORM = 0.05\nHEATMAP_IOU_THRESH = 0.05\n# --------------------------------------------------\n\n# ---------------- logging & font -------------------\ndef log(msg: str):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n\ndef get_font(px=14):\n    try:\n        return ImageFont.truetype(\"DejaVuSans.ttf\", px)\n    except Exception:\n        try:\n            return ImageFont.truetype(\"arial.ttf\", px)\n        except Exception:\n            return ImageFont.load_default()\n\nFONT = get_font(14)\n# --------------------------------------------------\n\n# ----------------- helpers -------------------------\ndef run_openface(video_path: str, outdir: str):\n    os.makedirs(outdir, exist_ok=True)\n    cmd = f'{OPENFACE_BIN} -f \"{video_path}\" -out_dir \"{outdir}\" -2Dfp -pose -gaze'\n    log(f\"Running OpenFace (may take a while): {cmd}\")\n    rc = os.system(cmd)\n    if rc != 0:\n        log(f\"WARNING: OpenFace returned non-zero exit code ({rc}). If CSVs exist in outdir, you can continue.\")\n\ndef find_openface_csv(video_path: str, outdir: str) -> Optional[str]:\n    base = os.path.splitext(os.path.basename(video_path))[0]\n    cands = sorted(glob.glob(os.path.join(outdir, \"*.csv\")))\n    # prefer csv containing base name\n    filtered = [p for p in cands if base in os.path.basename(p)]\n    if filtered:\n        return filtered[0]\n    # else return most recently modified CSV\n    return cands[0] if cands else None\n\nCOL_MAP = {\n    \"frame\": [\"frame\", \"frameNumber\", \"frame_num\"],\n    \"face_x\": [\"face_x\", \"face_x_px\", \"face_xpos\"],\n    \"face_y\": [\"face_y\", \"face_y_px\", \"face_ypos\"],\n    \"face_w\": [\"face_w\", \"face_width\", \"face_x_size\"],\n    \"face_h\": [\"face_h\", \"face_height\", \"face_y_size\"],\n    # fallback landmarks\n    \"lx\": [\"x_36\", \"landmark_36_x\", \"X_36\"],\n    \"ly\": [\"y_36\", \"landmark_36_y\", \"Y_36\"],\n    \"gax\": [\"gaze_angle_x\", \"GazeAngleX\", \"gaze_angle_x(deg)\"],\n    \"gay\": [\"gaze_angle_y\", \"GazeAngleY\", \"gaze_angle_y(deg)\"],\n}\ndef find_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n    for c in candidates:\n        if c in df.columns:\n            return c\n    return None\n\ndef detect_columns(df: pd.DataFrame) -> Dict[str, Optional[str]]:\n    out = {}\n    for k, cand in COL_MAP.items():\n        out[k] = find_col(df, cand)\n    return out\n\ndef norm_box_from_px(x, y, w, h, W, H):\n    xmin = max(0.0, x) / float(W)\n    ymin = max(0.0, y) / float(H)\n    xmax = min(float(W), x + w) / float(W)\n    ymax = min(float(H), y + h) / float(H)\n    return [xmin, ymin, xmax, ymax]\n\ndef rotate_point_ccw90(pt: Tuple[float,float]) -> Optional[Tuple[float,float]]:\n    if pt is None: return None\n    x,y = pt\n    xc = x - 0.5; yc = y - 0.5\n    xr = -yc; yr = xc\n    return (xr + 0.5, yr + 0.5)\n\ndef rotate_box_ccw90(box: Optional[List[float]]) -> Optional[List[float]]:\n    if box is None: return None\n    xmin,ymin,xmax,ymax = box\n    corners = [(xmin,ymin),(xmin,ymax),(xmax,ymin),(xmax,ymax)]\n    rc = [rotate_point_ccw90(c) for c in corners]\n    xs = [c[0] for c in rc]; ys = [c[1] for c in rc]\n    return [max(0.0,min(xs)), max(0.0,min(ys)), min(1.0,max(xs)), min(1.0,max(ys))]\n\ndef clamp01(v): return max(0.0, min(1.0, float(v)))\n\ndef moving_average_points(points: List[Optional[Tuple[float,float]]], window: int):\n    if window <= 1: return points[:]\n    n=len(points); out=[]\n    half = window//2\n    for i in range(n):\n        s=max(0,i-half); e=min(n, i+half+1)\n        xs=[]; ys=[]\n        for j in range(s,e):\n            p = points[j]\n            if p is not None:\n                xs.append(p[0]); ys.append(p[1])\n        out.append((float(np.mean(xs)), float(np.mean(ys))) if xs else None)\n    return out\n\ndef normalized_distance(p1, p2):\n    return math.hypot(p1[0]-p2[0], p1[1]-p2[1])\n\ndef compute_fixations(points: List[Optional[Tuple[float,float]]], fps: float, radius_norm: float):\n    fix=[]\n    n=len(points); i=0\n    while i<n:\n        if points[i] is None:\n            i+=1; continue\n        s=i; pts=[points[i]]; j=i+1\n        while j<n and points[j] is not None and normalized_distance(points[j], points[s]) <= radius_norm:\n            pts.append(points[j]); j+=1\n        e = j-1\n        dur = (e - s + 1) / float(fps)\n        centroid = (float(np.mean([p[0] for p in pts])), float(np.mean([p[1] for p in pts])))\n        spread = float(np.mean([math.hypot(p[0]-centroid[0], p[1]-centroid[1]) for p in pts]))\n        fix.append((s, e, dur, centroid, spread))\n        i = j\n    return fix\n\ndef compute_shift_freq(points: List[Optional[Tuple[float,float]]], fps: float, motion_thresh_norm: float):\n    n=len(points)\n    if n==0: return 0.0\n    shifts=0; prev=None\n    for p in points:\n        if p is None:\n            prev=None; continue\n        if prev is not None and normalized_distance(prev,p) > motion_thresh_norm:\n            shifts += 1\n        prev = p\n    secs = n / float(fps)\n    return shifts / (secs + 1e-12)\n\ndef heatmap_to_rgba_img(hm, out_size, alpha=140):\n    if hm is None:\n        return None\n    arr = hm if isinstance(hm, np.ndarray) else np.array(hm)\n    # normalize\n    arr = arr - arr.min()\n    if arr.max() > 0:\n        arr = arr / (arr.max() + 1e-12)\n    else:\n        arr = arr * 0.0\n    img = Image.fromarray((arr * 255).astype(np.uint8))\n    try:\n        img = img.resize(out_size, resample=Image.Resampling.BILINEAR)\n    except Exception:\n        img = img.resize(out_size, resample=Image.BILINEAR)\n    cmap = cm.get_cmap(\"jet\")\n    cmaped = cmap(np.array(img) / 255.0)\n    rgba = (cmaped * 255).astype(np.uint8)\n    rgba_img = Image.fromarray(rgba, mode=\"RGBA\")\n    rgba_img.putalpha(alpha)\n    return rgba_img\n\ndef draw_overlay(pil: Image.Image, norm_bbox: Optional[List[float]], gaze_pt_norm: Optional[Tuple[float,float]],\n                 hm: Optional[np.ndarray]=None, label: Optional[str]=None, color=\"lime\"):\n    w,h = pil.size\n    out = pil.convert(\"RGBA\").copy()\n    draw = ImageDraw.Draw(out)\n    if hm is not None:\n        try:\n            hm_rgba = heatmap_to_rgba_img(hm, (w,h), alpha=140)\n            if hm_rgba is not None:\n                out = Image.alpha_composite(out, hm_rgba)\n                draw = ImageDraw.Draw(out)\n        except Exception:\n            pass\n    if norm_bbox:\n        rect = [norm_bbox[0]*w, norm_bbox[1]*h, norm_bbox[2]*w, norm_bbox[3]*h]\n        draw.rectangle(rect, outline=color, width=max(1,int(min(w,h)*0.006)))\n    if gaze_pt_norm is not None:\n        gx = gaze_pt_norm[0]*w; gy = gaze_pt_norm[1]*h\n        cx = ((norm_bbox[0] + norm_bbox[2]) / 2.0)*w if norm_bbox else w/2\n        cy = ((norm_bbox[1] + norm_bbox[3]) / 2.0)*h if norm_bbox else h/2\n        r = max(4, int(min(w,h)*0.008))\n        draw.ellipse([(gx-r,gy-r),(gx+r,gy+r)], fill=color)\n        draw.line([(cx,cy),(gx,gy)], fill=color, width=max(1,int(min(w,h)*0.006)))\n    if label:\n        draw.text((10,10), label, fill=\"white\", font=FONT)\n    return np.array(out.convert(\"RGB\"))\n\ndef find_heatmap_peak_xy(hm: np.ndarray) -> Tuple[float,float,float]:\n    # returns (gx_norm, gy_norm, peak_value)\n    if hm is None:\n        return (None, None, 0.0)\n    H,W = hm.shape\n    iy, ix = np.unravel_index(np.argmax(hm), hm.shape)\n    gx = float(ix) / float(W)\n    gy = float(iy) / float(H)\n    return (gx, gy, float(hm[iy, ix]))\n\ndef classify_gaze_context(gaze_pt, partner_box, workspace_y_min):\n    \"\"\"\n    Classifies gaze into:\n    - looking_to_partner\n    - looking_workspace\n    - looking_elsewhere\n    \"\"\"\n    if gaze_pt is None:\n        return \"looking_elsewhere\"\n\n    x, y = gaze_pt\n\n    # ---------- Check partner region ----------\n    if partner_box is not None:\n        xmin, ymin, xmax, ymax = partner_box\n        cx = (xmin + xmax) / 2.0\n        cy = (ymin + ymax) / 2.0\n\n        w = (xmax - xmin) * PARTNER_BOX_EXPAND\n        h = (ymax - ymin) * PARTNER_BOX_EXPAND\n\n        # expanded bounding box\n        px_min = max(0.0, cx - w / 2.0)\n        px_max = min(1.0, cx + w / 2.0)\n        py_min = max(0.0, cy - h / 2.0)\n        py_max = min(1.0, cy + h / 2.0)\n\n        if px_min <= x <= px_max and py_min <= y <= py_max:\n            return \"looking_to_partner\"\n\n    # ---------- workspace region (bottom of screen) ----------\n    if y >= workspace_y_min:\n        return \"looking_workspace\"\n\n    # ---------- otherwise ----------\n    return \"looking_elsewhere\"\n# --------------------------------------------------\n\n# --------------- process one video ----------------\ndef process_video(video_path: str, of_csv: str, model, transform, device, label=\"P\"):\n    log(f\"[{label}] loading frames from {video_path}\")\n    frames = list(iio.imiter(video_path, plugin=\"FFMPEG\"))\n    meta = iio.immeta(video_path, plugin=\"FFMPEG\")\n    fps = float(meta.get(\"fps\", 25.0))\n    if len(frames) == 0:\n        raise RuntimeError(f\"No frames read from {video_path}\")\n    H, W = frames[0].shape[0], frames[0].shape[1]\n\n    df = pd.read_csv(of_csv)\n    cols = detect_columns(df)\n\n    recs = []  # each: dict with frame_idx, norm_bbox, gaze_pt, heatmap, inout, face_px bbox (optional)\n    log(f\"[{label}] processing {min(len(frames), len(df))} frames (csv rows vs frames)\")\n    n = min(len(frames), len(df))\n    for i in range(n):\n        row = df.iloc[i]\n        # compute face bbox normalized\n        nb = None\n        try:\n            if cols[\"face_x\"] and cols[\"face_y\"] and cols[\"face_w\"] and cols[\"face_h\"]:\n                fx = float(row[cols[\"face_x\"]]); fy = float(row[cols[\"face_y\"]])\n                fw = float(row[cols[\"face_w\"]]); fh = float(row[cols[\"face_h\"]])\n                nb = norm_box_from_px(fx,fy,fw,fh,W,H)\n        except Exception:\n            nb = None\n        # fallback to landmark\n        if nb is None and cols[\"lx\"] and cols[\"ly\"]:\n            try:\n                lx = float(row[cols[\"lx\"]]); ly = float(row[cols[\"ly\"]])\n                cx = lx / float(W); cy = ly / float(H)\n                bw,bh = 0.2, 0.30\n                nb = [clamp01(cx - bw/2), clamp01(cy - bh/2), clamp01(cx + bw/2), clamp01(cy + bh/2)]\n            except Exception:\n                nb = None\n\n        pil = Image.fromarray(frames[i])\n        gaze_pt = None; hm_np = None; inout_val = None; peak_val = 0.0\n        if nb is not None:\n            # call gazelle model\n            with torch.no_grad():\n                img_t = transform(pil).unsqueeze(0).to(device)\n                inp = {\"images\": img_t, \"bboxes\": [[nb]]}\n                out = model(inp)\n            # heatmap extraction (handle list or tensor shape)\n            hm = out.get(\"heatmap\", None)\n            if hm is None:\n                hm_np = None\n            else:\n                # hm may be list or tensor: prefer hm[0][0] or hm[0,0]\n                if isinstance(hm, list):\n                    hm_t = hm[0][0]\n                else:\n                    # shape may be (B, C, H, W)\n                    hm_t = hm[0,0]\n                hm_np = hm_t.detach().cpu().numpy()\n                gx, gy, peak_val = find_heatmap_peak_xy(hm_np)\n                inout_arr = out.get(\"inout\", None)\n                try:\n                    if inout_arr is not None:\n                        # out[\"inout\"][0] could be tensor shape (1,) or list\n                        v = inout_arr[0]\n                        inout_val = float(v.detach().cpu().numpy()) if hasattr(v, \"detach\") else float(v)\n                except Exception:\n                    inout_val = None\n                # decide gaze point visible\n                if inout_val is None or inout_val > INOUT_THRESH:\n                    gaze_pt = (gx, gy)\n                else:\n                    gaze_pt = None\n\n        recs.append({\n            \"frame_idx\": i,\n            \"norm_bbox\": nb,\n            \"gaze_pt\": gaze_pt,\n            \"heatmap\": hm_np,\n            \"inout\": inout_val,\n            # face pixel bbox saved for convenience\n            \"frame_bbox_px\": None\n        })\n\n    return frames[:n], fps, recs\n\n# --------- feature computation & outputs ----------\ndef compute_and_save_all(v1, v2, outdir):\n    os.makedirs(outdir, exist_ok=True)\n\n    # Ensure OpenFace CSVs exist (run OpenFace first)\n    log(\"Running OpenFace to generate CSVs (if needed). This can be skipped if you already have CSVs.\")\n    run_openface(v1, outdir)\n    run_openface(v2, outdir)\n    csv1 = find_openface_csv(v1, outdir)\n    csv2 = find_openface_csv(v2, outdir)\n    if csv1 is None or csv2 is None:\n        log(\"ERROR: Could not find OpenFace CSVs. Look in outdir and re-run.\")\n        log(f\"Files in {outdir}: {sorted(glob.glob(os.path.join(outdir,'*')))}\")\n        sys.exit(1)\n    log(f\"Found OpenFace CSVs:\\n P1 -> {csv1}\\n P2 -> {csv2}\")\n\n    # Load model (GazeLLE) from torch.hub\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    log(f\"Loading GazeLLE via torch.hub (device={device}) ...\")\n    model, transform = torch.hub.load(\"fkryan/gazelle\", \"gazelle_dinov2_vitl14_inout\")\n    model.to(device).eval()\n    log(\"Model loaded.\")\n\n    # Process both videos\n    frames1, fps1, rec1 = process_video(v1, csv1, model, transform, device, label=\"P1\")\n    frames2, fps2, rec2 = process_video(v2, csv2, model, transform, device, label=\"P2\")\n    n_frames = min(len(rec1), len(rec2))\n    fps = min(float(fps1), float(fps2))\n    log(f\"Processing summary: frames P1={len(rec1)}, P2={len(rec2)} -> using n_frames={n_frames}, fps≈{fps:.2f}\")\n\n    # Extract smoothed gaze trajectories\n    gp1 = [rec1[i][\"gaze_pt\"] for i in range(n_frames)]\n    gp2 = [rec2[i][\"gaze_pt\"] for i in range(n_frames)]\n    gp1_sm = moving_average_points(gp1, SMOOTH_WINDOW)\n    gp2_sm = moving_average_points(gp2, SMOOTH_WINDOW)\n\n    # Fixations and shifts\n    fix1 = compute_fixations(gp1_sm, fps, FIXATION_RADIUS_NORM)\n    fix2 = compute_fixations(gp2_sm, fps, FIXATION_RADIUS_NORM)\n    avg_fix1 = float(np.mean([f[2] for f in fix1])) if len(fix1)>0 else 0.0\n    avg_fix2 = float(np.mean([f[2] for f in fix2])) if len(fix2)>0 else 0.0\n    num_fix1 = len(fix1); num_fix2 = len(fix2)\n    avg_spread1 = float(np.mean([f[4] for f in fix1])) if len(fix1)>0 else 0.0\n    avg_spread2 = float(np.mean([f[4] for f in fix2])) if len(fix2)>0 else 0.0\n    shift_freq1 = compute_shift_freq(gp1_sm, fps, MOTION_THRESH_NORM)\n    shift_freq2 = compute_shift_freq(gp2_sm, fps, MOTION_THRESH_NORM)\n\n    # Align P2 into P1/world (rotate 90° CCW) for joint metrics\n    gp2_al = [rotate_point_ccw90(p) if p is not None else None for p in gp2_sm]\n    nb1 = [rec1[i][\"norm_bbox\"] for i in range(n_frames)]\n    nb2 = [rec2[i][\"norm_bbox\"] for i in range(n_frames)]\n    nb2_al = [rotate_box_ccw90(b) if b is not None else None for b in nb2]\n    hm1 = [rec1[i][\"heatmap\"] for i in range(n_frames)]\n    hm2_al = [np.rot90(rec2[i][\"heatmap\"], k=1) if rec2[i][\"heatmap\"] is not None else None for i in range(n_frames)]\n\n    # Contexts\n    ctx1 = [classify_gaze_context(gp1_sm[i], nb2_al[i], WORKSPACE_Y_MIN) for i in range(n_frames)]\n    ctx2 = [classify_gaze_context(gp2_sm[i], None, WORKSPACE_Y_MIN) for i in range(n_frames)]\n\n    # Joint frames: distance between gp1_sm and gp2_al <= threshold\n    joint_flags = [ (gp1_sm[i] is not None and gp2_al[i] is not None and normalized_distance(gp1_sm[i], gp2_al[i]) <= JOINT_GAZE_DISTANCE_NORM) for i in range(n_frames) ]\n    joint_frames = sum(1 for f in joint_flags if f)\n    joint_time_sec = joint_frames / (fps + 1e-12)\n\n    # Shared Attention IoU (heatmap overlap approx)\n    iou_list = []\n    for i in range(n_frames):\n        a1 = hm1[i]; a2 = hm2_al[i]\n        if a1 is None or a2 is None:\n            iou_list.append(0.0); continue\n        # thresholded binary maps\n        t1 = a1.max() * HEATMAP_IOU_THRESH\n        t2 = a2.max() * HEATMAP_IOU_THRESH\n        b1 = (a1 >= t1).astype(np.uint8)\n        b2 = (a2 >= t2).astype(np.uint8)\n        inter = np.logical_and(b1,b2).sum()\n        uni = np.logical_or(b1,b2).sum()\n        iou_list.append(float(inter)/float(uni) if uni>0 else 0.0)\n    mean_iou = float(np.mean(iou_list)) if len(iou_list)>0 else 0.0\n\n    # episodes of shared focus\n    episodes=[]\n    i=0\n    while i < n_frames:\n        if joint_flags[i]:\n            s=i; j=i+1\n            while j<n_frames and joint_flags[j]: j+=1\n            e=j-1\n            episodes.append((s,e,(e-s+1)/fps))\n            i=j\n        else:\n            i+=1\n    num_episodes = len(episodes)\n    avg_episode_dur = float(np.mean([ep[2] for ep in episodes])) if episodes else 0.0\n\n    # Mutual gaze: both looking_to_partner\n    mutual_frames = sum(1 for i in range(n_frames) if ctx1[i]==\"looking_to_partner\" and ctx2[i]==\"looking_to_partner\")\n    mutual_time = mutual_frames / (fps + 1e-12)\n\n    # percent times\n    def pct(lst, tag): return float(lst.count(tag)) / float(n_frames) * 100.0\n    pct_workspace1 = pct(ctx1, \"looking_workspace\"); pct_partner1 = pct(ctx1, \"looking_to_partner\"); pct_else1 = pct(ctx1, \"looking_elsewhere\")\n    pct_workspace2 = pct(ctx2, \"looking_workspace\"); pct_partner2 = pct(ctx2, \"looking_to_partner\"); pct_else2 = pct(ctx2, \"looking_elsewhere\")\n\n    # gaze direction stats (angle from face center to gaze point)\n    def direction_angles(nb_list, gp_list):\n        angs=[]\n        for i in range(n_frames):\n            nb = nb_list[i]; gp = gp_list[i]\n            if nb is None or gp is None: continue\n            cx = (nb[0]+nb[2])/2.0; cy = (nb[1]+nb[3])/2.0\n            dx = gp[0] - cx; dy = gp[1] - cy\n            ang = (math.degrees(math.atan2(dy, dx)) + 360.0) % 360.0\n            angs.append(ang)\n        return angs\n    angs1 = direction_angles(nb1, gp1_sm)\n    angs2 = direction_angles(nb2_al, gp2_al)\n    avg_angle1 = float(np.mean(angs1)) if angs1 else 0.0\n    std_angle1 = float(np.std(angs1)) if angs1 else 0.0\n    avg_angle2 = float(np.mean(angs2)) if angs2 else 0.0\n    std_angle2 = float(np.std(angs2)) if angs2 else 0.0\n\n    # Task Engagement Score (TES) & Social Engagement Score (SES)\n    # TES = %workspace / (%elsewhere + 1)  (keeps scale, avoid div by zero)\n    TES1 = pct_workspace1 / (pct_else1 + 1.0)\n    TES2 = pct_workspace2 / (pct_else2 + 1.0)\n    # SES = %TimeLookingAtPartner / 100  (0..1)\n    SES1 = pct_partner1 / 100.0\n    SES2 = pct_partner2 / 100.0\n\n    # Build feature matrix rows\n    fm_header = [\n        \"Person\",\n        \"Focus Duration on One Object (s)\",\n        \"Gaze Shifts per Second\",\n        \"% Time Looking at Workspace\",\n        \"% Time Looking at Partner\",\n        \"% Time Looking Elsewhere\",\n        \"Average Gaze Direction (deg)\",\n        \"Gaze Direction Variability (deg)\",\n        \"Number of Fixations\",\n        \"Average Focus Area Spread\",\n        \"Time Both Look at the Same Region (s)\",\n        \"Shared Attention Overlap Ratio\",\n        \"Number of Shared Focus Episodes\",\n        \"Average Duration per Shared Focus (s)\",\n        \"Mutual Gaze Duration (s)\",\n        \"Task Engagement Score\",\n        \"Social Engagement Score\"\n    ]\n    fm_rows = [\n        [\n            \"P1\",\n            round(avg_fix1,4),\n            round(shift_freq1,4),\n            round(pct_workspace1,3),\n            round(pct_partner1,3),\n            round(pct_else1,3),\n            round(avg_angle1,3),\n            round(std_angle1,3),\n            int(num_fix1),\n            round(avg_spread1,6),\n            round(joint_time_sec,4),\n            round(mean_iou,6),\n            int(num_episodes),\n            round(avg_episode_dur,4),\n            round(mutual_time,4),\n            round(TES1,4),\n            round(SES1,4)\n        ],\n        [\n            \"P2\",\n            round(avg_fix2,4),\n            round(shift_freq2,4),\n            round(pct_workspace2,3),\n            round(pct_partner2,3),\n            round(pct_else2,3),\n            round(avg_angle2,3),\n            round(std_angle2,3),\n            int(num_fix2),\n            round(avg_spread2,6),\n            round(joint_time_sec,4),\n            round(mean_iou,6),\n            int(num_episodes),\n            round(avg_episode_dur,4),\n            round(mutual_time,4),\n            round(TES2,4),\n            round(SES2,4)\n        ]\n    ]\n    # Save feature matrix\n    fm_path = os.path.join(outdir, \"p_17_feature_matrix.csv\")\n    with open(fm_path, \"w\", newline=\"\") as f:\n        w = csv.writer(f); w.writerow(fm_header); w.writerows(fm_rows)\n    log(f\"Saved feature_matrix: {fm_path}\")\n\n    # individual summary (first 10 columns)\n    ind_header = fm_header[:10]\n    ind_path = os.path.join(outdir, \"p_17_gaze_individual_summary.csv\")\n    with open(ind_path, \"w\", newline=\"\") as f:\n        w = csv.writer(f); w.writerow(ind_header); w.writerows(fm_rows)\n    log(f\"Saved individual summary: {ind_path}\")\n\n    # pairwise summary\n    pair_header = [\n        \"Time Both Look at the Same Region (s)\",\n        \"Shared Attention Overlap Ratio\",\n        \"Number of Shared Focus Episodes\",\n        \"Average Duration per Shared Focus (s)\",\n        \"Mutual Gaze Duration (s)\"\n    ]\n    pair_row = [ round(joint_time_sec,4), round(mean_iou,6), int(num_episodes), round(avg_episode_dur,4), round(mutual_time,4) ]\n    pair_path = os.path.join(outdir, \"p_17_gaze_pairwise_summary.csv\")\n    with open(pair_path, \"w\", newline=\"\") as f:\n        w = csv.writer(f); w.writerow(pair_header); w.writerow(pair_row)\n    log(f\"Saved pairwise summary: {pair_path}\")\n\n    # Per-frame pair records (logs) — includes contexts and gaze points and inout\n    records = []\n    for i in range(n_frames):\n        t = i / (fps + 1e-12)\n        records.append({\n            \"frame\": i,\n            \"time_s\": round(t, 3),\n            \"P1_context\": ctx1[i],\n            \"P2_context\": ctx2[i],\n            \"P1_gaze_x\": round(gp1_sm[i][0], 6) if gp1_sm[i] is not None else None,\n            \"P1_gaze_y\": round(gp1_sm[i][1], 6) if gp1_sm[i] is not None else None,\n            \"P2_gaze_x\": round(gp2_sm[i][0], 6) if gp2_sm[i] is not None else None,\n            \"P2_gaze_y\": round(gp2_sm[i][1], 6) if gp2_sm[i] is not None else None,\n            \"P1_inout\": rec1[i][\"inout\"],\n            \"P2_inout\": rec2[i][\"inout\"],\n            \"joint_flag\": bool(joint_flags[i]),\n            \"heatmap_iou\": round(iou_list[i], 6)\n        })\n    recs_df = pd.DataFrame(records)\n    recs_path = os.path.join(outdir, \"p_17_pair_gaze_records.csv\")\n    recs_df.to_csv(recs_path, index=False)\n    log(f\"Saved per-frame pair records: {recs_path}\")\n\n    # Filtered event logs: moments when each person is looking_to_partner or looking_elsewhere\n    def events_for_person(ctx_list, gp_sm, recs, person_tag):\n        ev = []\n        for i, ctx in enumerate(ctx_list):\n            if ctx in (\"looking_to_partner\", \"looking_elsewhere\"):\n                t = i / (fps + 1e-12)\n                ev.append({\n                    \"frame\": i,\n                    \"time_s\": round(t, 3),\n                    \"context\": ctx,\n                    \"gaze_x\": round(gp_sm[i][0],6) if gp_sm[i] is not None else None,\n                    \"gaze_y\": round(gp_sm[i][1],6) if gp_sm[i] is not None else None,\n                    \"inout\": recs[i][\"inout\"],\n                    \"bbox\": recs[i][\"norm_bbox\"]\n                })\n        return ev\n    ev1 = events_for_person(ctx1, gp1_sm, rec1, \"P1\")\n    ev2 = events_for_person(ctx2, gp2_sm, rec2, \"P2\")\n    pd.DataFrame(ev1).to_csv(os.path.join(outdir, \"p_17_p1_partner_elsewhere_events.csv\"), index=False)\n    pd.DataFrame(ev2).to_csv(os.path.join(outdir, \"p_17_p2_partner_elsewhere_events.csv\"), index=False)\n    log(f\"Saved per-person event files: p1_partner_elsewhere_events.csv, p2_partner_elsewhere_events.csv\")\n\n    # Annotated videos (draw heatmap + gaze point + label)\n    log(\"Rendering annotated videos (this can take time)...\")\n    ann1 = []\n    ann2 = []\n    for i in range(n_frames):\n        pil1 = Image.fromarray(frames1[i])\n        pil2 = Image.fromarray(frames2[i])\n        hm1_i = rec1[i][\"heatmap\"]\n        hm2_i = rec2[i][\"heatmap\"]\n        label1 = f\"P1: {ctx1[i]}\"\n        label2 = f\"P2: {ctx2[i]}\"\n        img1 = draw_overlay(pil1, rec1[i][\"norm_bbox\"], gp1_sm[i], hm=hm1_i, label=label1, color=\"lime\")\n        img2 = draw_overlay(pil2, rec2[i][\"norm_bbox\"], gp2_sm[i], hm=hm2_i, label=label2, color=\"cyan\")\n        ann1.append(img1)\n        ann2.append(img2)\n        if (i % 200) == 0 and i>0:\n            log(f\" Annotating frame {i}/{n_frames}...\")\n\n    out_v1 = os.path.join(outdir, \"p_17_annotated_p1.mp4\")\n    out_v2 = os.path.join(outdir, \"p_17_annotated_p2.mp4\")\n    # write as mp4\n    iio.imwrite(out_v1, ann1, fps=fps, codec=\"libx264\", quality=8)\n    iio.imwrite(out_v2, ann2, fps=fps, codec=\"libx264\", quality=8)\n    log(f\"Saved annotated videos:\\n {out_v1}\\n {out_v2}\")\n\n    return {\n        \"feature_matrix\": fm_path,\n        \"individual_summary\": ind_path,\n        \"pairwise_summary\": pair_path,\n        \"pair_records\": recs_path,\n        \"events_p1\": os.path.join(outdir, \"p_17_p1_partner_elsewhere_events.csv\"),\n        \"events_p2\": os.path.join(outdir, \"p_17_p2_partner_elsewhere_events.csv\"),\n        \"annotated_videos\": (out_v1, out_v2)\n    }\n\n# -------------------- main -------------------------\nif __name__ == \"__main__\":\n    # sanity checks\n    if not os.path.exists(OPENFACE_BIN):\n        log(f\"OpenFace binary not found at: {OPENFACE_BIN}\")\n        log(\"Please build OpenFace and set OPENFACE_BIN to your FeatureExtraction path.\")\n        sys.exit(1)\n    if not os.path.exists(VIDEO1) or not os.path.exists(VIDEO2):\n        log(\"Set VIDEO1/VIDEO2 paths at the top of the script and re-run.\")\n        sys.exit(1)\n    os.makedirs(OUTDIR, exist_ok=True)\n    t0 = time.time()\n    res = compute_and_save_all(VIDEO1, VIDEO2, OUTDIR)\n    t1 = time.time()\n    log(\"All done.\")\n    log(f\"Outputs: {res}\")\n    log(f\"Elapsed: {t1 - t0:.1f} s\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}