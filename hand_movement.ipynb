{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13614993,"sourceType":"datasetVersion","datasetId":8652545},{"sourceId":13622429,"sourceType":"datasetVersion","datasetId":8657641}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mediapipe --quiet\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:00:30.607965Z","iopub.execute_input":"2025-11-14T22:00:30.608117Z","iopub.status.idle":"2025-11-14T22:00:52.491472Z","shell.execute_reply.started":"2025-11-14T22:00:30.608102Z","shell.execute_reply":"2025-11-14T22:00:52.490785Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 4.25.8 which is incompatible.\ngoogle-cloud-bigtable 2.32.0 requires google-api-core[grpc]<3.0.0,>=2.17.0, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# final_pipeline_hsv_ready_to_run.py\n# Full pipeline: MediaPipe hands + HSV object detection (color masks)\n# Diagonal person split, robust grasp/touch/reach detection, timeouts, merges\n# Outputs: annotated video, JSON log, per-person feature CSV, anomaly summary, debug log\n\nimport cv2\nimport numpy as np\nimport mediapipe as mp\nimport json, time, math, csv, os\nfrom collections import defaultdict, deque, Counter\n\n# ---------------- CONFIG ----------------\ninput_path = \"/kaggle/input/videoss/VIDEO/p_17/webcam_output.mp4\"   # change if needed\noutput_path = \"annotated_output_hsv_final_ready_p_17.mp4\"\nlog_json_path = \"interaction_log_hsv_final_ready_p17.json\"\nfeature_csv_path = \"feature_matrix_hsv_final_ready_p17.csv\"\nanomaly_json_path = \"anomaly_summary_hsv_final_ready_p17.json\"\ndebug_log_path = \"debug_log_hsv_final_ready_p17.txt\"\n\nFPS = 30\nLOG_EVERY_N_FRAMES = 100\nSHOW_PREVIEW = False\n\n# thresholds and hyperparams\nMIN_OBJ_AREA = 80.0\nGRASP_FRAMES = 30\nGRASP_HOLD_TOLERANCE_FRAMES = 10\nGRASP_TIMEOUT_FRAMES = 45\nMERGE_GAP_FRAMES = 30\nSTART_COOLDOWN_FRAMES = 10\nREACH_THRESHOLD = 100\nMAX_HANDS = 4\nSMOOTH_HISTORY = 15\nMAX_HANDS_PER_PERSON = 2\nHAND_OBJECT_IOU_THRESH = 0.7\n\n# diagonal constants (locked scalars)\nM_val = float(-0.5)\nC_val = float(500.0)\n\n# HSV color ranges\nHSV_RANGES = {\n    \"red\": [ (np.array([0,120,120]), np.array([10,255,255])),\n             (np.array([160,120,120]), np.array([180,255,255])) ],\n    \"green\": [(np.array([40,70,70]), np.array([80,255,255]))],\n    \"yellow\": [(np.array([20,120,150]), np.array([35,255,255]))],\n    \"white\": [(np.array([0,0,180]), np.array([180,50,255]))],\n    \"gray\": [(np.array([0,0,50]), np.array([180,30,180]))]\n}\n\nDRAW_COLOR = {\n    \"red\": (0,0,255), \"green\": (0,255,0), \"yellow\": (0,255,255),\n    \"gray\": (128,128,128), \"white\": (255,255,255), \"unknown\": (200,200,200),\n    \"diag\": (255,0,0), \"hand_bbox\": (120,120,120), \"text\": (255,255,255)\n}\n\n# ---------------- logging helper ----------------\ndef log_write(msg, level=\"INFO\"):\n    ts = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n    line = f\"{ts} [{level}] {msg}\"\n    # write only to file (no continuous console printing)\n    with open(debug_log_path, \"a\") as f:\n        f.write(line + \"\\n\")\n\ndef print_progress(frame_idx):\n    if frame_idx % LOG_EVERY_N_FRAMES == 0:\n        print(f\"Processed {frame_idx} frames...\")\n\nif os.path.exists(debug_log_path): os.remove(debug_log_path)\nlog_write(\"Starting FINAL HSV pipeline (ready to run)\", \"INFO\")\n\n# ---------------- helpers ----------------\ndef rect_center(rect):\n    x1,y1,x2,y2 = rect\n    return ((x1+x2)/2.0, (y1+y2)/2.0)\n\ndef iou(a,b):\n    xA = max(a[0], b[0]); yA = max(a[1], b[1])\n    xB = min(a[2], b[2]); yB = min(a[3], b[3])\n    interW = max(0, xB-xA); interH = max(0, yB-yA)\n    inter = interW * interH\n    union = max(1, (a[2]-a[0])*(a[3]-a[1]) + (b[2]-b[0])*(b[3]-b[1]) - inter)\n    return inter / union\n\ndef object_overlap_with_hand(obj_bb, hand_bb):\n    xA = max(obj_bb[0], hand_bb[0]); yA = max(obj_bb[1], hand_bb[1])\n    xB = min(obj_bb[2], hand_bb[2]); yB = min(obj_bb[3], hand_bb[3])\n    interW = max(0, xB-xA); interH = max(0, yB-yA)\n    inter = interW * interH\n    obj_area = max(1, (obj_bb[2]-obj_bb[0])*(obj_bb[3]-obj_bb[1]))\n    return inter / obj_area\n\ndef dist(a,b):\n    return math.hypot(a[0]-b[0], a[1]-b[1])\n\n# ---------------- CentroidTracker ----------------\nclass CentroidTracker:\n    def __init__(self, max_disappeared=40):\n        self.next_id = 1\n        self.tracks = {}\n        self.max_disappeared = max_disappeared\n    def register(self, bbox, frame_idx):\n        tid = self.next_id; self.next_id += 1\n        self.tracks[tid] = {'bbox': bbox, 'disappeared': 0, 'last_seen': frame_idx}\n        return tid\n    def deregister(self, tid):\n        if tid in self.tracks: del self.tracks[tid]\n    def update(self, rects, frame_idx):\n        if len(rects) == 0:\n            for tid in list(self.tracks.keys()):\n                self.tracks[tid]['disappeared'] += 1\n                if self.tracks[tid]['disappeared'] > self.max_disappeared:\n                    self.deregister(tid)\n            return {tid: self.tracks[tid]['bbox'] for tid in self.tracks}\n        if len(self.tracks) == 0:\n            for r in rects: self.register(r, frame_idx)\n            return {tid: v['bbox'] for tid,v in self.tracks.items()}\n        tids = list(self.tracks.keys())\n        existing = [rect_center(self.tracks[t]['bbox']) for t in tids]\n        inputs = [rect_center(r) for r in rects]\n        cost = np.zeros((len(existing), len(inputs)), dtype=np.float32)\n        for i,e in enumerate(existing):\n            for j,inp in enumerate(inputs):\n                cost[i,j] = dist(e, inp)\n        pairs = sorted([(i,j,cost[i,j]) for i in range(cost.shape[0]) for j in range(cost.shape[1])], key=lambda x: x[2])\n        used_r, used_c = set(), set()\n        for i,j,_ in pairs:\n            if i in used_r or j in used_c: continue\n            used_r.add(i); used_c.add(j)\n            tid = tids[i]\n            self.tracks[tid]['bbox'] = rects[j]\n            self.tracks[tid]['disappeared'] = 0\n            self.tracks[tid]['last_seen'] = frame_idx\n        unmatched_inputs = [j for j in range(len(inputs)) if j not in used_c]\n        unmatched_existing = [i for i in range(len(existing)) if i not in used_r]\n        for j in unmatched_inputs: self.register(rects[j], frame_idx)\n        for i in unmatched_existing:\n            tid = tids[i]; self.tracks[tid]['disappeared'] += 1\n            if self.tracks[tid]['disappeared'] > self.max_disappeared: self.deregister(tid)\n        return {tid: self.tracks[tid]['bbox'] for tid in self.tracks}\n\n# ---------------- MediaPipe & trackers ----------------\nmp_hands = mp.solutions.hands\nmp_draw = mp.solutions.drawing_utils\nhands = mp_hands.Hands(static_image_mode=False,\n                       max_num_hands=MAX_HANDS,\n                       model_complexity=1,\n                       min_detection_confidence=0.5,\n                       min_tracking_confidence=0.5)\n\nobj_tracker = CentroidTracker(max_disappeared=40)\nhand_tracker = CentroidTracker(max_disappeared=40)\n\n# state\nhand_palm_smooth = defaultdict(lambda: deque(maxlen=SMOOTH_HISTORY))\ngrasp_counters = defaultdict(int)\ninteraction_log = []\nactive_interactions = {}\ninteraction_miss_counts = defaultdict(int)\nholding_state = {}\nobject_shared_state = defaultdict(lambda: False)\nobject_pass_count = defaultdict(int)\n\nhand_positions_by_person = defaultdict(list)\nhand_prev_pos_by_tid = {}\nhand_speed_samples = defaultdict(list)\nhand_distance_sums = defaultdict(float)\nhand_distance_counts = defaultdict(int)\n\nframes_with_any_interaction_by_person = defaultdict(int)\nframes_with_both_hands_active_by_person = defaultdict(int)\nframes_present_by_person = defaultdict(int)\nobject_shared_frames = defaultdict(int)\ncolor_counts_by_person = defaultdict(lambda: defaultdict(int))\n\ninteraction_sessions = []\nhand_person_history = defaultdict(lambda: deque(maxlen=SMOOTH_HISTORY))\nhand_slot_history = {0: {1: None, 2: None}, 1: {1: None, 2: None}}\nlast_known_hand_label = {}\nstart_cooldown = defaultdict(lambda: -9999)\n\n# ---------------- Video IO ----------------\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise RuntimeError(f\"Cannot open video: {input_path}\")\nfps = FPS\nW = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nH = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\nout = cv2.VideoWriter(output_path, fourcc, fps, (W, H))\nframe_idx = 0\nlog_write(f\"Video opened: {input_path} ({W}x{H}) fps={fps}\", \"INFO\")\n\n# ---------------- Utilities reused ----------------\ndef get_smoothed_person_id_for_hand(htid, palm_xy):\n    x = float(palm_xy[0]); y = float(palm_xy[1])\n    raw_pid = 0 if (y > (M_val * x + C_val)) else 1\n    hand_person_history[htid].append(raw_pid)\n    counts = Counter(hand_person_history[htid])\n    most_common = counts.most_common()\n    if not most_common: return raw_pid\n    top_count = most_common[0][1]\n    top_items = [item for item,count in most_common if count==top_count]\n    if len(top_items) == 1: return top_items[0]\n    else: return hand_person_history[htid][-1]\n\ndef distance_to_diagonal(point):\n    x,y = float(point[0]), float(point[1])\n    return abs(M_val * x - y + C_val) / math.sqrt(M_val * M_val + 1.0)\n\ndef rebalance_hands_by_diagonal(hand_to_person, smoothed_palm):\n    per_pid = {0: [], 1: []}\n    for tid, pid in hand_to_person.items():\n        if pid in (0,1): per_pid[pid].append(tid)\n    changed = True\n    while changed:\n        changed = False\n        for pid in (0,1):\n            tids = per_pid[pid]\n            if len(tids) > MAX_HANDS_PER_PERSON:\n                def get_pos(tid):\n                    if tid in smoothed_palm and smoothed_palm[tid] is not None: return smoothed_palm[tid]\n                    return np.array([0.0,0.0], dtype=np.float32)\n                move_tid = min(tids, key=lambda t: distance_to_diagonal(get_pos(t)))\n                other = 1 - pid\n                if len(per_pid[other]) < MAX_HANDS_PER_PERSON:\n                    hand_to_person[move_tid] = other\n                    tids.remove(move_tid)\n                    per_pid[other].append(move_tid)\n                else:\n                    hand_to_person[move_tid] = -1\n                    tids.remove(move_tid)\n                changed = True\n    return hand_to_person\n\ndef assign_hand_slots(hand_to_person, smoothed_palm):\n    per_pid = {0: [], 1: []}\n    for tid,pid in hand_to_person.items():\n        if pid in (0,1): per_pid[pid].append(tid)\n    hand_slot_labels = {}\n    for pid in (0,1):\n        tids = per_pid[pid]\n        sorted_by_x = sorted(tids, key=lambda t: float(smoothed_palm.get(t, (0,0))[0]))\n        prev_map = hand_slot_history[pid]\n        used_slots = set()\n        for slot_idx in (1,2):\n            prev_tid = prev_map.get(slot_idx)\n            if prev_tid in sorted_by_x:\n                hand_slot_labels[prev_tid] = f\"H{slot_idx}\"\n                used_slots.add(slot_idx)\n                sorted_by_x.remove(prev_tid)\n        slot_candidates = [s for s in (1,2) if s not in used_slots]\n        for tid, slot_idx in zip(sorted_by_x, slot_candidates):\n            hand_slot_labels[tid] = f\"H{slot_idx}\"; used_slots.add(slot_idx)\n        for slot_idx in (1,2):\n            assigned_tid = None\n            for tid, lab in hand_slot_labels.items():\n                if lab == f\"H{slot_idx}\" and hand_to_person.get(tid)==pid:\n                    assigned_tid = tid\n            hand_slot_history[pid][slot_idx] = assigned_tid\n    return hand_slot_labels\n\n# ---------------- HSV detection ----------------\ndef detect_hsv_objects(frame, min_area_local=MIN_OBJ_AREA):\n    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n    kernel = np.ones((3,3), np.uint8)\n    detected = []\n    for cname, ranges in HSV_RANGES.items():\n        mask = None\n        for lo, hi in ranges:\n            msk = cv2.inRange(hsv, lo, hi)\n            mask = msk if mask is None else cv2.bitwise_or(mask, msk)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        for c in contours:\n            area = cv2.contourArea(c)\n            if area < min_area_local: continue\n            x,y,w,h = cv2.boundingRect(c)\n            detected.append({\"bbox\":[x,y,x+w,y+h], \"color\":cname, \"area\":float(area)})\n    return detected\n\n# ---------------- main loop ----------------\ntry:\n    while True:\n        ret, frame = cap.read()\n        if not ret: break\n        frame_idx += 1\n        time_s = frame_idx / fps\n\n        if frame_idx % LOG_EVERY_N_FRAMES == 0:\n            print_progress(frame_idx)\n            log_write(f\"Frame {frame_idx} | active_interactions={len(active_interactions)} holding={len(holding_state)} objects={len(obj_tracker.tracks)}\", \"INFO\")\n\n        # 1) hands via MediaPipe\n        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        res = hands.process(rgb)\n        hand_rects = []; hand_landmarks_list = []; hand_labels_list = []\n        if res.multi_hand_landmarks:\n            handedness_info = res.multi_handedness or []\n            for i, lm in enumerate(res.multi_hand_landmarks):\n                xs = [p.x * W for p in lm.landmark]\n                ys = [p.y * H for p in lm.landmark]\n                bb = [int(min(xs)), int(min(ys)), int(max(xs)), int(max(ys))]\n                hand_rects.append(bb)\n                hand_landmarks_list.append(lm)\n                lab = handedness_info[i].classification[0].label if i < len(handedness_info) else \"Unknown\"\n                hand_labels_list.append(lab)\n\n        # 2) update hand tracker\n        tracked_hands = hand_tracker.update(hand_rects, frame_idx)\n\n        # 3) map landmarks -> hand_info_by_tid and smooth palms\n        hand_info_by_tid = {}\n        for htid, hb in tracked_hands.items():\n            best_iou = 0; best_idx = None\n            for idx, hr in enumerate(hand_rects):\n                v = iou(hb, hr)\n                if v > best_iou:\n                    best_iou = v; best_idx = idx\n            if best_idx is not None and best_iou > 0:\n                lm = hand_landmarks_list[best_idx]\n                palm = np.array([lm.landmark[0].x * W, lm.landmark[0].y * H])\n                tips = [np.array([lm.landmark[i].x * W, lm.landmark[i].y * H]) for i in [4,8,12,16,20]]\n                label = hand_labels_list[best_idx] if best_idx < len(hand_labels_list) else \"Unknown\"\n                if label == \"Unknown\" and htid in last_known_hand_label:\n                    label = last_known_hand_label[htid]\n                else:\n                    last_known_hand_label[htid] = label\n                hand_info_by_tid[htid] = {'bbox': hb, 'palm': palm, 'tips': tips, 'label': label}\n                hand_palm_smooth[htid].append(palm)\n            else:\n                cx, cy = rect_center(hb)\n                palm = np.array([cx, cy])\n                label = last_known_hand_label.get(htid, \"Unknown\")\n                hand_info_by_tid[htid] = {'bbox': hb, 'palm': palm, 'tips': [], 'label': label}\n                hand_palm_smooth[htid].append(palm)\n\n        smoothed_palm = {}\n        for tid, dq in hand_palm_smooth.items():\n            arr = np.array(dq)\n            if arr.size == 0: continue\n            smoothed_palm[tid] = arr.mean(axis=0)\n\n        # 4) assign person ids per hand\n        hand_to_person = {}\n        for tid in list(hand_info_by_tid.keys()):\n            p = smoothed_palm.get(tid, np.array(rect_center(hand_info_by_tid[tid]['bbox'])))\n            pid_smoothed = get_smoothed_person_id_for_hand(tid, p)\n            hand_to_person[tid] = pid_smoothed\n\n        # 5) rebalance hands\n        hand_to_person = rebalance_hands_by_diagonal(hand_to_person, smoothed_palm)\n\n        # 6) assign hand slots\n        hand_slot_labels = assign_hand_slots(hand_to_person, smoothed_palm)\n\n        # 7) HSV object detection + filter objects on hands\n        detected_objs_raw = detect_hsv_objects(frame, min_area_local=MIN_OBJ_AREA)\n        filtered_objs = []\n        for obj in detected_objs_raw:\n            obb = obj[\"bbox\"]\n            keep = True\n            for h_bb in hand_rects:\n                overlap = object_overlap_with_hand(obb, h_bb)\n                if overlap >= HAND_OBJECT_IOU_THRESH:\n                    keep = False\n                    break\n            if keep: filtered_objs.append(obj)\n\n        rects_full = [o['bbox'] for o in filtered_objs]\n\n        # 8) update object tracker\n        tracked_objects = obj_tracker.update(rects_full, frame_idx)\n        tracked_info = {}\n        for tid, tb in tracked_objects.items():\n            best_color=\"unknown\"; best_iou_val = 0.0\n            for o in filtered_objs:\n                v = iou(tb, o['bbox'])\n                if v > best_iou_val:\n                    best_iou_val = v; best_color = o['color']\n            tracked_info[tid] = {'bbox': tb, 'color': best_color}\n\n        # 9) store hand positions and speeds\n        for htid, hinfo in hand_info_by_tid.items():\n            pid = hand_to_person.get(htid, -1)\n            if pid not in (0,1): continue\n            p = smoothed_palm.get(htid, np.array(hinfo['palm']))\n            hand_positions_by_person[pid].append((float(p[0]), float(p[1])))\n            frames_present_by_person[pid] += 1\n            prev = hand_prev_pos_by_tid.get(htid)\n            if prev is not None:\n                s = dist(prev, p)\n                hand_speed_samples[pid].append(s)\n            hand_prev_pos_by_tid[htid] = (float(p[0]), float(p[1]))\n            if tracked_info:\n                obj_centers = [rect_center(v['bbox']) for v in tracked_info.values()]\n                dists = [dist(p, oc) for oc in obj_centers]\n                min_d = min(dists) if dists else None\n                if min_d is not None:\n                    hand_distance_sums[pid] += min_d\n                    hand_distance_counts[pid] += 1\n\n        # 10) interaction detection & update states\n        object_fingertips_by_person = defaultdict(lambda: defaultdict(int))\n        observed_pairs_this_frame = set()\n        for htid, hinfo in hand_info_by_tid.items():\n            palm = smoothed_palm.get(htid, np.array(hinfo['palm']))\n            tips = hinfo['tips']\n            pid = hand_to_person.get(htid, -1)\n            hand_label = hinfo.get('label','Unknown')\n            if pid not in (0,1): continue\n\n            for oid, oinfo in tracked_info.items():\n                obb = oinfo['bbox']; color = oinfo['color']\n                fingertips_inside = 0\n                for t in tips:\n                    if obb[0] <= t[0] <= obb[2] and obb[1] <= t[1] <= obb[3]:\n                        fingertips_inside += 1\n                if fingertips_inside > 0:\n                    object_fingertips_by_person[oid][pid] += fingertips_inside\n\n                palm_dist = float(np.linalg.norm(palm - np.array(rect_center(obb))))\n                typ = \"NO_INTERACTION\"\n                if fingertips_inside >= 3 or palm_dist < min(80, REACH_THRESHOLD * 0.5):\n                    typ = \"GRASPING_CANDIDATE\"\n                elif obb[0] <= palm[0] <= obb[2] and obb[1] <= palm[1] <= obb[3]:\n                    typ = \"TOUCHING\"\n                elif palm_dist < REACH_THRESHOLD:\n                    typ = \"REACHING\"\n                else:\n                    typ = \"NO_INTERACTION\"\n\n                key = (htid, oid)\n                observed_pairs_this_frame.add(key)\n                interaction_miss_counts[key] = 0\n\n                if typ == \"GRASPING_CANDIDATE\":\n                    grasp_counters[key] += 1\n                else:\n                    grasp_counters[key] = max(0, grasp_counters.get(key, 0) - 1)\n                confirmed_grasp = grasp_counters.get(key, 0) >= GRASP_FRAMES\n                final_typ = typ\n                if typ == \"GRASPING_CANDIDATE\" and confirmed_grasp:\n                    final_typ = \"GRASPING\"\n\n                prev = active_interactions.get(key)\n                last_start = start_cooldown.get(key, -9999)\n                can_start = (frame_idx - last_start) > START_COOLDOWN_FRAMES\n\n                if final_typ != \"NO_INTERACTION\" and final_typ != \"GRASPING_CANDIDATE\":\n                    if prev is None and can_start:\n                        active_interactions[key] = {'type': final_typ, 'start_frame': frame_idx, 'start_time': frame_idx/fps,\n                                                   'hand_label': hand_label, 'person_id': pid, 'object_tid': oid, 'last_seen_frame': frame_idx}\n                        start_cooldown[key] = frame_idx\n                        interaction_log.append({'event':'interaction_start','frame': frame_idx, 'time_s': frame_idx/fps,\n                                                'type': final_typ, 'hand_label': hand_label, 'person_id': pid, 'object_tid': oid})\n                        log_write(f\"interaction_start hand={htid} obj={oid} type={final_typ} pid={pid} frame={frame_idx}\", \"INFO\")\n                    else:\n                        if prev is not None:\n                            prev['last_seen_frame'] = frame_idx\n                            if prev['type'] != final_typ and can_start:\n                                # change type -> end previous, start new\n                                s_prev = prev\n                                # end previous\n                                end_rec = active_interactions.pop(key)\n                                duration_s = max(0.0, (frame_idx - 1 - end_rec['start_frame'])/fps)\n                                interaction_sessions.append({'type': end_rec['type'], 'start_frame': end_rec['start_frame'], 'end_frame': frame_idx-1, 'duration_s': duration_s, 'hand_label': end_rec.get('hand_label','Unknown'), 'person_id': end_rec.get('person_id', -1), 'object_tid': end_rec.get('object_tid', -1)})\n                                interaction_log.append({'event':'interaction_end','start_frame': end_rec['start_frame'], 'end_frame': frame_idx-1, 'duration_s': duration_s, 'type': end_rec['type'], 'hand_label': end_rec.get('hand_label','Unknown'), 'person_id': end_rec.get('person_id', -1), 'object_tid': end_rec.get('object_tid', -1)})\n                                # start new\n                                active_interactions[key] = {'type': final_typ, 'start_frame': frame_idx, 'start_time': frame_idx/fps,\n                                                           'hand_label': hand_label, 'person_id': pid, 'object_tid': oid, 'last_seen_frame': frame_idx}\n                                start_cooldown[key] = frame_idx\n                                interaction_log.append({'event':'interaction_start','frame': frame_idx, 'time_s': frame_idx/fps,\n                                                        'type': final_typ, 'hand_label': hand_label, 'person_id': pid, 'object_tid': oid})\n                                log_write(f\"interaction_restart hand={htid} obj={oid} type={final_typ} pid={pid} frame={frame_idx}\", \"INFO\")\n\n                    pk = f\"p{pid}_o{oid}\"\n                    if final_typ == \"GRASPING\":\n                        if pk not in holding_state:\n                            holding_state[pk] = {\"start_frame\": frame_idx, \"start_time\": frame_idx/fps, \"person_id\": pid, \"object_tid\": oid, \"object_color\": color, \"hand_label\": hand_label}\n                            interaction_log.append({'event':'pick_up','frame': frame_idx, 'time_s': frame_idx/fps, 'person_id': pid, 'object_tid': oid, 'object_color': color, 'hand_label': hand_label})\n                            color_counts_by_person[pid][color] += 1\n                            log_write(f\"pick_up person={pid} obj={oid} frame={frame_idx}\", \"INFO\")\n                    else:\n                        if pk in holding_state and final_typ != \"GRASPING\":\n                            st = holding_state.pop(pk)\n                            end_frame = frame_idx\n                            duration_s = max(0.0, (end_frame - st['start_frame']) / fps)\n                            interaction_log.append({'event':'put_down','start_frame': st['start_frame'], 'end_frame': end_frame, 'duration_s': duration_s, 'person_id': st['person_id'], 'object_tid': st['object_tid'], 'object_color': st['object_color'], 'end_hand_label': hand_label})\n                            log_write(f\"put_down person={st['person_id']} obj={st['object_tid']} dur={duration_s:.3f} frame={frame_idx}\", \"INFO\")\n                else:\n                    if prev is not None:\n                        # end interaction if previously active but now no interaction\n                        end_rec = active_interactions.pop(key)\n                        duration_s = max(0.0, (frame_idx - 1 - end_rec['start_frame']) / fps)\n                        interaction_sessions.append({'type': end_rec['type'], 'start_frame': end_rec['start_frame'], 'end_frame': frame_idx-1, 'duration_s': duration_s, 'hand_label': end_rec.get('hand_label','Unknown'), 'person_id': end_rec.get('person_id', -1), 'object_tid': end_rec.get('object_tid', -1)})\n                        interaction_log.append({'event':'interaction_end','start_frame': end_rec['start_frame'], 'end_frame': frame_idx-1, 'duration_s': duration_s, 'type': end_rec['type'], 'hand_label': end_rec.get('hand_label','Unknown'), 'person_id': end_rec.get('person_id', -1), 'object_tid': end_rec.get('object_tid', -1)})\n                        log_write(f\"interaction_end hand={htid} obj={oid} dur={duration_s:.3f} at frame={frame_idx-1}\", \"INFO\")\n\n        # 11) pass/shared detection\n        for oid in tracked_info.keys():\n            persons_with_fingers = [pid for pid in object_fingertips_by_person.get(oid, {}) if object_fingertips_by_person[oid][pid] > 0]\n            shared_now = (0 in persons_with_fingers) and (1 in persons_with_fingers)\n            if shared_now and not object_shared_state[oid]:\n                object_shared_state[oid] = True\n                object_pass_count[oid] += 1\n                interaction_log.append({'event':'object_shared_start','frame': frame_idx, 'time_s': frame_idx/fps, 'object_tid': oid})\n                log_write(f\"object_shared_start obj={oid} frame={frame_idx}\", \"INFO\")\n            if not shared_now and object_shared_state[oid]:\n                object_shared_state[oid] = False\n                interaction_log.append({'event':'object_shared_end','frame': frame_idx, 'time_s': frame_idx/fps, 'object_tid': oid})\n                log_write(f\"object_shared_end obj={oid} frame={frame_idx}\", \"INFO\")\n            if shared_now:\n                object_shared_frames[oid] += 1\n\n        # 12) per-frame active hands per person\n        active_hands_per_person = defaultdict(set)\n        for key, rec in active_interactions.items():\n            pid = rec.get('person_id', -1)\n            lab = rec.get('hand_label', 'Unknown')\n            if pid >= 0:\n                active_hands_per_person[pid].add(lab)\n        for pid in [0,1]:\n            if len(active_hands_per_person.get(pid, set())) > 0:\n                frames_with_any_interaction_by_person[pid] += 1\n            if len(active_hands_per_person.get(pid, set())) >= 2:\n                frames_with_both_hands_active_by_person[pid] += 1\n\n        # 13) timeout handling for missing interactions\n        for key in list(active_interactions.keys()):\n            if key not in observed_pairs_this_frame:\n                interaction_miss_counts[key] += 1\n                if interaction_miss_counts[key] > GRASP_TIMEOUT_FRAMES:\n                    rec = active_interactions.pop(key)\n                    s_frame = rec['start_frame']; e_frame = frame_idx - 1\n                    duration_s = max(0.0, (e_frame - s_frame) / fps)\n                    interaction_sessions.append({'type': rec['type'], 'start_frame': s_frame, 'end_frame': e_frame, 'duration_s': duration_s, 'hand_label': rec.get('hand_label','Unknown'), 'person_id': rec.get('person_id', -1), 'object_tid': rec.get('object_tid', -1)})\n                    interaction_log.append({'event':'interaction_end','start_frame': s_frame, 'end_frame': e_frame, 'duration_s': duration_s, 'type': rec['type'], 'hand_label': rec.get('hand_label','Unknown'), 'person_id': rec.get('person_id', -1), 'object_tid': rec.get('object_tid', -1), 'end_reason':'timeout'})\n                    log_write(f\"interaction_end (timeout) key={key} dur={duration_s:.3f} frame={frame_idx}\", \"INFO\")\n                    if key in interaction_miss_counts: del interaction_miss_counts[key]\n            else:\n                interaction_miss_counts[key] = 0\n\n        # 14) draw overlays\n        pt1 = (0, int(C_val)); pt2 = (W, int(M_val * W + C_val))\n        cv2.line(frame, pt1, pt2, DRAW_COLOR['diag'], 1)\n        cv2.putText(frame, \"Side A: pid=0\", (10,20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, DRAW_COLOR['text'], 1)\n        cv2.putText(frame, \"Side B: pid=1\", (W-150,20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, DRAW_COLOR['text'], 1)\n\n        for oid, oinfo in tracked_info.items():\n            bb = oinfo['bbox']; col = DRAW_COLOR.get(oinfo['color'], DRAW_COLOR['unknown'])\n            x0,y0,x3,y3 = map(int, bb)\n            cv2.rectangle(frame, (x0,y0), (x3,y3), col, 2)\n            cv2.putText(frame, f\"{oinfo['color']}_O{oid}\", (x0, max(y0-8,10)), cv2.FONT_HERSHEY_SIMPLEX, 0.45, col, 1)\n\n        for htid, hinfo in hand_info_by_tid.items():\n            hb = hinfo['bbox']; label = hinfo.get('label','Unknown')\n            x0,y0,x3,y3 = map(int, hb)\n            pid = hand_to_person.get(htid, -1)\n            slot_label = hand_slot_labels.get(htid, \"H?\")\n            cv2.rectangle(frame, (x0,y0), (x3,y3), DRAW_COLOR['hand_bbox'], 1)\n            pid_text = f\"P{pid}\" if pid in (0,1) else \"P?\"\n            display_label = f\"{label} {slot_label} {pid_text}\"\n            cv2.putText(frame, display_label, (x0, max(y0-8,10)), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (200,200,0), 1)\n            if htid in smoothed_palm:\n                px,py = map(int, smoothed_palm[htid])\n                cv2.circle(frame, (px,py), 4, (200,200,0), -1)\n\n        out.write(frame)\n        if SHOW_PREVIEW:\n            cv2.imshow(\"frame\", frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n\nfinally:\n    # finalize active interactions\n    for key in list(active_interactions.keys()):\n        rec = active_interactions.pop(key)\n        s_frame = rec['start_frame']; e_frame = frame_idx\n        duration_s = max(0.0, (e_frame - s_frame) / fps)\n        interaction_sessions.append({'type': rec['type'], 'start_frame': s_frame, 'end_frame': e_frame, 'duration_s': duration_s, 'hand_label': rec.get('hand_label','Unknown'), 'person_id': rec.get('person_id', -1), 'object_tid': rec.get('object_tid', -1)})\n        interaction_log.append({'event':'interaction_end','start_frame': s_frame, 'end_frame': e_frame, 'duration_s': duration_s, 'type': rec['type'], 'hand_label': rec.get('hand_label','Unknown'), 'person_id': rec.get('person_id', -1), 'object_tid': rec.get('object_tid', -1), 'end_reason':'finalize'})\n    for pk, st in list(holding_state.items()):\n        end_frame = frame_idx\n        duration_s = max(0.0, (end_frame - st['start_frame']) / fps)\n        interaction_log.append({'event':'put_down','start_frame': st['start_frame'], 'end_frame': end_frame, 'duration_s': duration_s, 'person_id': st['person_id'], 'object_tid': st['object_tid'], 'object_color': st.get('object_color','Unknown'), 'end_hand_label': st.get('hand_label','Unknown')})\n    cap.release(); out.release()\n    if SHOW_PREVIEW: cv2.destroyAllWindows()\n    hands.close()\n\nlog_write(\"Processing finished. Computing features...\", \"INFO\")\n\n# ---------------- Feature computations ----------------\ntotal_interactions = len(interaction_sessions)\nnum_reaches = len([s for s in interaction_sessions if s['type'].lower().startswith('reach')])\nnum_touches = len([s for s in interaction_sessions if s['type'].lower().startswith('touch')])\nnum_grasps = len([s for s in interaction_sessions if s['type'].lower().startswith('grasp')])\nunique_objects_interacted = len(set([s['object_tid'] for s in interaction_sessions if s['object_tid']>=0]))\n\ninteraction_durations = [s['duration_s'] for s in interaction_sessions if s['duration_s']>0]\navg_interaction_duration = float(np.mean(interaction_durations)) if interaction_durations else 0.0\n\n# --- Average duration per interaction type (global) ---\nreach_durations = [s['duration_s'] for s in interaction_sessions if s['type'].lower().startswith('reach') and s['duration_s']>0]\ntouch_durations = [s['duration_s'] for s in interaction_sessions if s['type'].lower().startswith('touch') and s['duration_s']>0]\ngrasp_durations_all = [s['duration_s'] for s in interaction_sessions if s['type'].lower().startswith('grasp') and s['duration_s']>0]\n\navg_reach_duration_global = float(np.mean(reach_durations)) if reach_durations else 0.0\navg_touch_duration_global = float(np.mean(touch_durations)) if touch_durations else 0.0\navg_grasp_duration_global = float(np.mean(grasp_durations_all)) if grasp_durations_all else 0.0\n\nstarts_sorted = sorted(interaction_sessions, key=lambda x: x['start_frame'])\ngaps = []\nfor i in range(len(starts_sorted)-1):\n    this_end = starts_sorted[i]['end_frame']; next_start = starts_sorted[i+1]['start_frame']\n    gaps.append(max(0.0, (next_start - this_end)/fps))\navg_interaction_gap = float(np.mean(gaps)) if gaps else 0.0\n\ngrasp_sessions = [s for s in interaction_sessions if s['type'].lower().startswith('grasp')]\ngrasp_sessions = sorted(grasp_sessions, key=lambda x: (x.get('person_id',-1), x.get('object_tid',-1), x['start_frame']))\nmerged_grasps = []\nfor s in grasp_sessions:\n    if not merged_grasps:\n        merged_grasps.append(dict(s)); continue\n    last = merged_grasps[-1]\n    if s.get('person_id') == last.get('person_id') and s.get('object_tid') == last.get('object_tid') and (s['start_frame'] - last['end_frame']) <= MERGE_GAP_FRAMES:\n        last['end_frame'] = s['end_frame']\n        last['duration_s'] = (last['end_frame'] - last['start_frame']) / fps\n    else:\n        merged_grasps.append(dict(s))\n\ngrasp_durations = [g['duration_s'] for g in merged_grasps]\navg_grasp_duration_all = float(np.mean(grasp_durations)) if grasp_durations else 0.0\nmax_grasp_duration_all = float(np.max(grasp_durations)) if grasp_durations else 0.0\nmin_grasp_duration_all = float(np.min(grasp_durations)) if grasp_durations else 0.0\n\ntotal_object_pass_count = int(sum(object_pass_count.values()))\ntotal_object_shared_frames = int(sum(object_shared_frames.values()))\n\navg_hand_speed = {}\nhand_speed_var = {}\nfor pid in [0,1]:\n    samples = hand_speed_samples.get(pid, [])\n    if len(samples) > 0:\n        avg_px_per_frame = float(np.mean(samples))\n        avg_hand_speed[pid] = float(avg_px_per_frame * fps)\n        hand_speed_var[pid] = float(np.std(samples) * fps)\n    else:\n        avg_hand_speed[pid] = 0.0\n        hand_speed_var[pid] = 0.0\n\nhand_label_counts = defaultdict(int)\nfor s in interaction_sessions:\n    lbl = s.get('hand_label','Unknown'); hand_label_counts[lbl] += 1\ndominant_hand_overall = max(hand_label_counts.items(), key=lambda x: x[1])[0] if hand_label_counts else \"Unknown\"\n\ndef convex_area_and_poly(pts):\n    if len(pts) < 3: return 0.0, None\n    arr = np.array(pts, dtype=np.float32)\n    hull = cv2.convexHull(arr)\n    area = float(cv2.contourArea(hull))\n    return area, hull\n\narea0, hull0 = convex_area_and_poly(hand_positions_by_person.get(0, []))\narea1, hull1 = convex_area_and_poly(hand_positions_by_person.get(1, []))\nworkspace_overlap_ratio = 0.0\nif hull0 is not None and hull1 is not None and area0>0 and area1>0:\n    try:\n        from shapely.geometry import Polygon\n        poly0 = Polygon(hull0.reshape(-1,2)); poly1 = Polygon(hull1.reshape(-1,2))\n        inter = poly0.intersection(poly1).area; union = poly0.union(poly1).area\n        if union > 0: workspace_overlap_ratio = float(inter / union)\n    except Exception:\n        workspace_overlap_ratio = 0.0\n\ntotal_time_s = frame_idx / fps if frame_idx>0 else 0.0\ninteraction_frequency_per_min = (total_interactions / total_time_s * 60.0) if total_time_s>0 else 0.0\n\n# per-person features\nper_person_features = {}\nfor pid in [0,1]:\n    sess = [s for s in interaction_sessions if s.get('person_id')==pid]\n    total_interactions_p = len(sess)\n    grasp_count_p = len([s for s in sess if s['type'].lower().startswith('grasp')])\n    touch_count_p = len([s for s in sess if s['type'].lower().startswith('touch')])\n    reach_count_p = len([s for s in sess if s['type'].lower().startswith('reach')])\n    grasp_sess = [s for s in sess if s['type'].lower().startswith('grasp')]\n    grasp_durs = [s['duration_s'] for s in grasp_sess]\n    avg_grasp_dur = float(np.mean(grasp_durs)) if grasp_durs else 0.0\n    total_grasp_time_p = float(np.sum(grasp_durs)) if grasp_durs else 0.0\n    interaction_frequency_p = (total_interactions_p / total_time_s) if total_time_s>0 else 0.0\n    starts_sorted_p = sorted(sess, key=lambda x: x['start_frame'])\n    gaps_p = []\n    for i in range(len(starts_sorted_p)-1):\n        this_end = starts_sorted_p[i]['end_frame']; next_start = starts_sorted_p[i+1]['start_frame']\n        gaps_p.append(max(0.0, (next_start - this_end)/fps))\n    interaction_gap_mean_p = float(np.mean(gaps_p)) if gaps_p else 0.0\n    objs = [s['object_tid'] for s in sess if s.get('object_tid') is not None and s.get('object_tid')>=0]\n    unique_objects_touched_p = len(set(objs))\n    color_counts = color_counts_by_person.get(pid, {})\n    if color_counts:\n        vals = np.array(list(color_counts.values()), dtype=np.float64)\n        probs = vals / vals.sum()\n        color_preference_entropy = float(-np.sum(probs * np.log2(probs)))\n    else:\n        color_preference_entropy = 0.0\n    switch_count = 0; prev_obj = None\n    for s in sorted(sess, key=lambda x: x['start_frame']):\n        oid = s.get('object_tid', None)\n        if prev_obj is not None and oid is not None and oid != prev_obj:\n            switch_count += 1\n        prev_obj = oid\n    object_switch_rate = (switch_count / total_time_s * 60.0) if total_time_s>0 else 0.0\n    left_count = sum(1 for s in sess if s.get('hand_label','').lower().startswith('left'))\n    right_count = sum(1 for s in sess if s.get('hand_label','').lower().startswith('right'))\n    left_right_balance = (left_count / (right_count+left_count)) if (right_count+left_count)>0 else 0.0\n    delays = []\n    grouped_by_obj = defaultdict(list)\n    for s in sess:\n        grouped_by_obj[s.get('object_tid')].append(s)\n    for oid, lst in grouped_by_obj.items():\n        lefts = [x for x in lst if x.get('hand_label','').lower().startswith('left')]\n        rights = [x for x in lst if x.get('hand_label','').lower().startswith('right')]\n        for L in lefts:\n            for R in rights:\n                delays.append(abs((L['start_frame'] - R['start_frame'])/fps))\n    dominant_hand_delay = float(np.mean(delays)) if delays else 0.0\n    frames_with_any = frames_with_any_interaction_by_person.get(pid, 0)\n    frames_with_both = frames_with_both_hands_active_by_person.get(pid, 0)\n    both_hands_active_ratio = (frames_with_both / frames_with_any) if frames_with_any>0 else 0.0\n    shared_frames_count = 0\n    person_object_ids = set([s.get('object_tid') for s in sess if s.get('object_tid') is not None])\n    for oid in person_object_ids:\n        shared_frames_count += object_shared_frames.get(oid, 0)\n    simultaneous_object_touch_ratio = (shared_frames_count / frames_with_any) if frames_with_any>0 else 0.0\n    avg_reach_distance = hand_distance_sums.get(pid, 0.0) / max(1, hand_distance_counts.get(pid,0)) if hand_distance_counts.get(pid,0)>0 else 0.0\n    workspace_area_pid = 0.0\n    pts = hand_positions_by_person.get(pid, [])\n    if len(pts) >= 3:\n        arr = np.array(pts, dtype=np.float32)\n        hull = cv2.convexHull(arr)\n        workspace_area_pid = float(cv2.contourArea(hull))\n    workspace_coverage_ratio = (workspace_area_pid / (W*H)) if (W*H)>0 else 0.0\n    avg_hand_speed_p = avg_hand_speed.get(pid, 0.0)\n    hand_speed_variability = hand_speed_var.get(pid, 0.0)\n    pickups = [ev for ev in interaction_log if ev.get('event')=='pick_up' and ev.get('person_id')==pid]\n    putdowns = [ev for ev in interaction_log if ev.get('event')=='put_down']\n    resp_times = []\n    for pu in pickups:\n        oid = pu.get('object_tid')\n        t_pick = pu.get('time_s', pu.get('frame', None)/fps if 'frame' in pu else None)\n        if oid is None or t_pick is None: continue\n        candidates = [pd for pd in putdowns if pd.get('object_tid')==oid]\n        if not candidates: continue\n        last_put = max(candidates, key=lambda x: x.get('time_s', x.get('frame',0)/fps))\n        t_put = last_put.get('time_s', last_put.get('frame',0)/fps)\n        if t_put is not None and t_pick >= t_put:\n            resp_times.append(t_pick - t_put)\n    interaction_response_time = float(np.mean(resp_times)) if resp_times else 0.0\n    pass_count_p = 0\n    for oid, cnt in object_pass_count.items():\n        if oid in person_object_ids: pass_count_p += cnt\n    frames_person = frames_with_any_interaction_by_person.get(pid,0)\n    frames_both_touch_same_time = sum(object_shared_frames.values())\n    cross_person_touch_overlap_ratio = (frames_both_touch_same_time / frames_person) if frames_person>0 else 0.0\n\n    # --- Average durations per interaction type for this person ---\n    reach_durs_p = [s['duration_s'] for s in sess if s['type'].lower().startswith('reach') and s['duration_s']>0]\n    touch_durs_p = [s['duration_s'] for s in sess if s['type'].lower().startswith('touch') and s['duration_s']>0]\n    grasp_durs_p = [s['duration_s'] for s in sess if s['type'].lower().startswith('grasp') and s['duration_s']>0]\n    all_durs_p = [s['duration_s'] for s in sess if s['duration_s']>0]\n\n    avg_reach_duration_p = float(np.mean(reach_durs_p)) if reach_durs_p else 0.0\n    avg_touch_duration_p = float(np.mean(touch_durs_p)) if touch_durs_p else 0.0\n    avg_grasp_duration_p = float(np.mean(grasp_durs_p)) if grasp_durs_p else 0.0\n    avg_interaction_duration_p = float(np.mean(all_durs_p)) if all_durs_p else 0.0\n\n    per_person_features[pid] = {\n        \"total_interactions\": int(total_interactions_p),\n        \"grasp_count\": int(grasp_count_p),\n        \"touch_count\": int(touch_count_p),\n        \"reach_count\": int(reach_count_p),\n        \"avg_grasp_duration\": float(avg_grasp_dur),\n        \"avg_reach_duration\": float(avg_reach_duration_p),\n        \"avg_touch_duration\": float(avg_touch_duration_p),\n        \"avg_interaction_duration\": float(avg_interaction_duration_p),\n        \"total_grasp_time\": float(total_grasp_time_p),\n        \"interaction_frequency\": float(interaction_frequency_p),\n        \"interaction_gap_mean\": float(interaction_gap_mean_p),\n        \"unique_objects_touched\": int(unique_objects_touched_p),\n        \"color_preference_entropy\": float(color_preference_entropy),\n        \"object_switch_rate\": float(object_switch_rate),\n        \"left_right_balance\": float(left_right_balance),\n        \"both_hands_active_ratio\": float(both_hands_active_ratio),\n        \"dominant_hand_delay\": float(dominant_hand_delay),\n        \"simultaneous_object_touch_ratio\": float(simultaneous_object_touch_ratio),\n        \"avg_reach_distance\": float(avg_reach_distance),\n        \"workspace_coverage_ratio\": float(workspace_coverage_ratio),\n        \"interaction_response_time\": float(interaction_response_time),\n        \"avg_hand_speed\": float(avg_hand_speed_p),\n        \"hand_speed_variability\": float(hand_speed_variability),\n        \"object_pass_count\": int(pass_count_p),\n        \"cross_person_touch_overlap_ratio\": float(cross_person_touch_overlap_ratio)\n    }\n\n# ---------------- Save outputs ----------------\nshared_meta = {\n    \"session_video\": input_path,\n    \"duration_s\": total_time_s,\n    \"fps\": fps,\n    \"total_interactions\": int(total_interactions),\n    \"num_reaches\": int(num_reaches),\n    \"num_touches\": int(num_touches),\n    \"num_grasps\": int(num_grasps),\n    \"unique_objects_interacted\": int(unique_objects_interacted),\n    \"avg_interaction_duration_s\": float(avg_interaction_duration),\n    \"avg_interaction_gap_s\": float(avg_interaction_gap),\n    \"avg_grasp_duration_s\": float(avg_grasp_duration_all),\n    \"max_grasp_duration_s\": float(max_grasp_duration_all),\n    \"min_grasp_duration_s\": float(min_grasp_duration_all),\n    \"object_pass_count\": int(total_object_pass_count),\n    \"total_object_shared_frames\": int(total_object_shared_frames),\n    \"interaction_frequency_per_min\": float(interaction_frequency_per_min),\n    \"dominant_hand_overall\": dominant_hand_overall,\n    \"workspace_overlap_ratio\": float(workspace_overlap_ratio),\n    \"frame_count\": int(frame_idx),\n    # new global averages per interaction type\n    \"avg_reach_duration_global\": float(avg_reach_duration_global),\n    \"avg_touch_duration_global\": float(avg_touch_duration_global),\n    \"avg_grasp_duration_global\": float(avg_grasp_duration_global),\n    \"avg_interaction_duration_global\": float(avg_interaction_duration)\n}\n\nsummary = {\n    \"meta\": {\n        \"processed_at\": time.time(),\n        \"fps\": fps,\n        \"GRASP_FRAMES\": GRASP_FRAMES,\n        \"REACH_THRESHOLD\": REACH_THRESHOLD,\n        \"diagonal_m\": M_val,\n        \"diagonal_c\": C_val\n    },\n    \"interaction_log\": interaction_log,\n    \"interaction_sessions\": interaction_sessions,\n    \"merged_grasps\": merged_grasps,\n    \"per_person_features_intermediate\": per_person_features,\n    \"shared_features\": shared_meta,\n    \"object_pass_count_detail\": dict(object_pass_count),\n    \"object_shared_frames\": dict(object_shared_frames),\n    \"color_counts_by_person\": {k: dict(v) for k,v in color_counts_by_person.items()}\n}\n\nwith open(log_json_path, \"w\") as f:\n    json.dump(summary, f, indent=2)\nlog_write(f\"Saved JSON log: {log_json_path}\", \"INFO\")\n\nanomaly_summary = {\n    \"frame_count\": frame_idx,\n    \"total_interaction_sessions\": len(interaction_sessions),\n    \"merged_grasp_count\": len(merged_grasps),\n    \"short_grasps_count\": len([g for g in merged_grasps if g['duration_s'] < 0.2]),\n    \"total_time_s\": total_time_s\n}\nwith open(anomaly_json_path, \"w\") as f:\n    json.dump(anomaly_summary, f, indent=2)\nlog_write(f\"Saved anomaly summary: {anomaly_json_path}\", \"INFO\")\n\n# write per-person CSV\ncolumns = [\n    \"session_video\",\"person_id\",\"duration_s\",\"fps\",\n    \"total_interactions\",\"grasp_count\",\"touch_count\",\"reach_count\",\n    \"avg_grasp_duration\",\"avg_reach_duration\",\"avg_touch_duration\",\"avg_interaction_duration\",\"total_grasp_time\",\"interaction_frequency\",\"interaction_gap_mean\",\n    \"unique_objects_touched\",\"color_preference_entropy\",\"object_switch_rate\",\n    \"left_right_balance\",\"both_hands_active_ratio\",\"dominant_hand_delay\",\"simultaneous_object_touch_ratio\",\n    \"avg_reach_distance\",\"workspace_coverage_ratio\",\n    \"interaction_response_time\",\"avg_hand_speed\",\"hand_speed_variability\",\"object_pass_count\",\"cross_person_touch_overlap_ratio\",\n    \"total_interactions_global\",\"num_reaches\",\"num_touches\",\"num_grasps\",\n    \"unique_objects_interacted\",\"avg_interaction_duration_s\",\"avg_interaction_gap_s\",\"object_pass_count_global\",\"workspace_overlap_ratio\",\"frame_count\",\n    # global averages\n    \"avg_reach_duration_global\",\"avg_touch_duration_global\",\"avg_grasp_duration_global\",\"avg_interaction_duration_global\"\n]\n\nwrite_header = not os.path.exists(feature_csv_path)\nwith open(feature_csv_path, \"a\", newline=\"\") as csvfile:\n    writer = csv.DictWriter(csvfile, fieldnames=columns)\n    if write_header: writer.writeheader()\n    for pid in [0,1]:\n        pf = per_person_features.get(pid, {})\n        row = {\n            \"session_video\": input_path,\n            \"person_id\": pid,\n            \"duration_s\": shared_meta[\"duration_s\"],\n            \"fps\": fps,\n            \"total_interactions\": pf.get(\"total_interactions\", 0),\n            \"grasp_count\": pf.get(\"grasp_count\", 0),\n            \"touch_count\": pf.get(\"touch_count\", 0),\n            \"reach_count\": pf.get(\"reach_count\", 0),\n            \"avg_grasp_duration\": pf.get(\"avg_grasp_duration\", 0.0),\n            \"avg_reach_duration\": pf.get(\"avg_reach_duration\", 0.0),\n            \"avg_touch_duration\": pf.get(\"avg_touch_duration\", 0.0),\n            \"avg_interaction_duration\": pf.get(\"avg_interaction_duration\", 0.0),\n            \"total_grasp_time\": pf.get(\"total_grasp_time\", 0.0),\n            \"interaction_frequency\": pf.get(\"interaction_frequency\", 0.0),\n            \"interaction_gap_mean\": pf.get(\"interaction_gap_mean\", 0.0),\n            \"unique_objects_touched\": pf.get(\"unique_objects_touched\", 0),\n            \"color_preference_entropy\": pf.get(\"color_preference_entropy\", 0.0),\n            \"object_switch_rate\": pf.get(\"object_switch_rate\", 0.0),\n            \"left_right_balance\": pf.get(\"left_right_balance\", 0.0),\n            \"both_hands_active_ratio\": pf.get(\"both_hands_active_ratio\", 0.0),\n            \"dominant_hand_delay\": pf.get(\"dominant_hand_delay\", 0.0),\n            \"simultaneous_object_touch_ratio\": pf.get(\"simultaneous_object_touch_ratio\", 0.0),\n            \"avg_reach_distance\": pf.get(\"avg_reach_distance\", 0.0),\n            \"workspace_coverage_ratio\": pf.get(\"workspace_coverage_ratio\", 0.0),\n            \"interaction_response_time\": pf.get(\"interaction_response_time\", 0.0),\n            \"avg_hand_speed\": pf.get(\"avg_hand_speed\", 0.0),\n            \"hand_speed_variability\": pf.get(\"hand_speed_variability\", 0.0),\n            \"object_pass_count\": pf.get(\"object_pass_count\", 0),\n            \"cross_person_touch_overlap_ratio\": pf.get(\"cross_person_touch_overlap_ratio\", 0.0),\n            \"total_interactions_global\": shared_meta[\"total_interactions\"],\n            \"num_reaches\": shared_meta[\"num_reaches\"],\n            \"num_touches\": shared_meta[\"num_touches\"],\n            \"num_grasps\": shared_meta[\"num_grasps\"],\n            \"unique_objects_interacted\": shared_meta[\"unique_objects_interacted\"],\n            \"avg_interaction_duration_s\": shared_meta[\"avg_interaction_duration_s\"],\n            \"avg_interaction_gap_s\": shared_meta[\"avg_interaction_gap_s\"],\n            \"object_pass_count_global\": shared_meta[\"object_pass_count\"],\n            \"workspace_overlap_ratio\": shared_meta[\"workspace_overlap_ratio\"],\n            \"frame_count\": shared_meta[\"frame_count\"],\n            # global averages\n            \"avg_reach_duration_global\": shared_meta.get(\"avg_reach_duration_global\", 0.0),\n            \"avg_touch_duration_global\": shared_meta.get(\"avg_touch_duration_global\", 0.0),\n            \"avg_grasp_duration_global\": shared_meta.get(\"avg_grasp_duration_global\", 0.0),\n            \"avg_interaction_duration_global\": shared_meta.get(\"avg_interaction_duration_global\", 0.0)\n        }\n        writer.writerow(row)\n\nlog_write(f\"Saved feature CSV: {feature_csv_path}\", \"INFO\")\nlog_write(\"Pipeline finished successfully ✅\", \"INFO\")\n\n# concise final prints\nprint(\"\\n✅ Processing complete. Outputs generated:\")\nprint(f\" - Annotated video: {output_path}\")\nprint(f\" - Full JSON log: {log_json_path}\")\nprint(f\" - Feature CSV: {feature_csv_path}\")\nprint(f\" - Anomaly JSON: {anomaly_json_path}\")\nprint(f\" - Debug log: {debug_log_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T13:42:25.054981Z","iopub.execute_input":"2025-11-14T13:42:25.055326Z","iopub.status.idle":"2025-11-14T13:57:00.708181Z","shell.execute_reply.started":"2025-11-14T13:42:25.055302Z","shell.execute_reply":"2025-11-14T13:57:00.707506Z"}},"outputs":[{"name":"stderr","text":"W0000 00:00:1763127745.410921     178 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1763127745.431388     177 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n","output_type":"stream"},{"name":"stdout","text":"Processed 100 frames...\nProcessed 200 frames...\nProcessed 300 frames...\nProcessed 400 frames...\nProcessed 500 frames...\nProcessed 600 frames...\nProcessed 700 frames...\nProcessed 800 frames...\nProcessed 900 frames...\nProcessed 1000 frames...\nProcessed 1100 frames...\nProcessed 1200 frames...\nProcessed 1300 frames...\nProcessed 1400 frames...\nProcessed 1500 frames...\nProcessed 1600 frames...\nProcessed 1700 frames...\nProcessed 1800 frames...\nProcessed 1900 frames...\nProcessed 2000 frames...\nProcessed 2100 frames...\nProcessed 2200 frames...\nProcessed 2300 frames...\nProcessed 2400 frames...\nProcessed 2500 frames...\nProcessed 2600 frames...\nProcessed 2700 frames...\nProcessed 2800 frames...\nProcessed 2900 frames...\nProcessed 3000 frames...\nProcessed 3100 frames...\nProcessed 3200 frames...\nProcessed 3300 frames...\nProcessed 3400 frames...\nProcessed 3500 frames...\nProcessed 3600 frames...\nProcessed 3700 frames...\nProcessed 3800 frames...\nProcessed 3900 frames...\nProcessed 4000 frames...\nProcessed 4100 frames...\nProcessed 4200 frames...\nProcessed 4300 frames...\nProcessed 4400 frames...\nProcessed 4500 frames...\nProcessed 4600 frames...\nProcessed 4700 frames...\nProcessed 4800 frames...\nProcessed 4900 frames...\nProcessed 5000 frames...\nProcessed 5100 frames...\nProcessed 5200 frames...\nProcessed 5300 frames...\nProcessed 5400 frames...\nProcessed 5500 frames...\nProcessed 5600 frames...\nProcessed 5700 frames...\nProcessed 5800 frames...\nProcessed 5900 frames...\nProcessed 6000 frames...\nProcessed 6100 frames...\nProcessed 6200 frames...\nProcessed 6300 frames...\nProcessed 6400 frames...\nProcessed 6500 frames...\nProcessed 6600 frames...\nProcessed 6700 frames...\nProcessed 6800 frames...\nProcessed 6900 frames...\nProcessed 7000 frames...\nProcessed 7100 frames...\nProcessed 7200 frames...\nProcessed 7300 frames...\nProcessed 7400 frames...\nProcessed 7500 frames...\nProcessed 7600 frames...\nProcessed 7700 frames...\nProcessed 7800 frames...\nProcessed 7900 frames...\nProcessed 8000 frames...\nProcessed 8100 frames...\nProcessed 8200 frames...\nProcessed 8300 frames...\nProcessed 8400 frames...\nProcessed 8500 frames...\nProcessed 8600 frames...\nProcessed 8700 frames...\nProcessed 8800 frames...\nProcessed 8900 frames...\nProcessed 9000 frames...\nProcessed 9100 frames...\nProcessed 9200 frames...\nProcessed 9300 frames...\nProcessed 9400 frames...\nProcessed 9500 frames...\nProcessed 9600 frames...\nProcessed 9700 frames...\nProcessed 9800 frames...\nProcessed 9900 frames...\nProcessed 10000 frames...\nProcessed 10100 frames...\nProcessed 10200 frames...\nProcessed 10300 frames...\nProcessed 10400 frames...\nProcessed 10500 frames...\nProcessed 10600 frames...\nProcessed 10700 frames...\nProcessed 10800 frames...\nProcessed 10900 frames...\nProcessed 11000 frames...\nProcessed 11100 frames...\nProcessed 11200 frames...\nProcessed 11300 frames...\nProcessed 11400 frames...\nProcessed 11500 frames...\nProcessed 11600 frames...\nProcessed 11700 frames...\nProcessed 11800 frames...\nProcessed 11900 frames...\nProcessed 12000 frames...\nProcessed 12100 frames...\nProcessed 12200 frames...\nProcessed 12300 frames...\nProcessed 12400 frames...\nProcessed 12500 frames...\nProcessed 12600 frames...\nProcessed 12700 frames...\nProcessed 12800 frames...\nProcessed 12900 frames...\nProcessed 13000 frames...\nProcessed 13100 frames...\nProcessed 13200 frames...\nProcessed 13300 frames...\nProcessed 13400 frames...\nProcessed 13500 frames...\nProcessed 13600 frames...\nProcessed 13700 frames...\nProcessed 13800 frames...\nProcessed 13900 frames...\nProcessed 14000 frames...\nProcessed 14100 frames...\nProcessed 14200 frames...\nProcessed 14300 frames...\nProcessed 14400 frames...\nProcessed 14500 frames...\nProcessed 14600 frames...\nProcessed 14700 frames...\nProcessed 14800 frames...\nProcessed 14900 frames...\nProcessed 15000 frames...\nProcessed 15100 frames...\nProcessed 15200 frames...\nProcessed 15300 frames...\nProcessed 15400 frames...\nProcessed 15500 frames...\nProcessed 15600 frames...\nProcessed 15700 frames...\nProcessed 15800 frames...\nProcessed 15900 frames...\nProcessed 16000 frames...\nProcessed 16100 frames...\nProcessed 16200 frames...\nProcessed 16300 frames...\nProcessed 16400 frames...\nProcessed 16500 frames...\nProcessed 16600 frames...\nProcessed 16700 frames...\nProcessed 16800 frames...\nProcessed 16900 frames...\nProcessed 17000 frames...\nProcessed 17100 frames...\nProcessed 17200 frames...\nProcessed 17300 frames...\nProcessed 17400 frames...\nProcessed 17500 frames...\nProcessed 17600 frames...\nProcessed 17700 frames...\nProcessed 17800 frames...\nProcessed 17900 frames...\n\n✅ Processing complete. Outputs generated:\n - Annotated video: annotated_output_hsv_final_ready_p_17.mp4\n - Full JSON log: interaction_log_hsv_final_ready_p17.json\n - Feature CSV: feature_matrix_hsv_final_ready_p17.csv\n - Anomaly JSON: anomaly_summary_hsv_final_ready_p17.json\n - Debug log: debug_log_hsv_final_ready_p17.txt\n","output_type":"stream"}],"execution_count":9}]}